{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"student_solution.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ThX_n6d7zwKQ","colab_type":"text"},"source":["<br>\n","<div class=\"alert alert-info\">\n","    <b> <h1> Life Insurance case study </h1></b>\n","</div>\n","\n","\n","https://www.kaggle.com/c/prudential-life-insurance-assessment/data"]},{"cell_type":"markdown","metadata":{"id":"xteZEqH6zwKR","colab_type":"text"},"source":["\n","## Introduction\n","\n","Siham, one of the largest issuers of life insurance in the Morocco, wants to develop an on-line the life insurance application process. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams. Siham wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries.\n","\n","By developing a predictive model that accurately classifies risk using a more automated approach, you can greatly help Siham to better understand the predictive power of the data points in the existing assessment, enabling it to significantly streamline the process.\n","\n","### Data : \n","\n","|  Num \t|   Name\t|   Values\t|\n","|-------|-------|-------|\n","|  1 \t|  `Id` \t|   int\t|\n","|  2\t|   `Product_Info_1`\t|  real \t|\n","|  3 \t|   `Product_Info_2`\t|  'D3', 'A1', 'E1', 'D4', 'D2', 'A8', 'A2', 'D1', 'A7', 'A6', 'A3','A5', 'C4', 'C1', 'B2', 'C3', 'C2', 'A4', 'B1' \t|\n","|  4 \t|   `Product_Info_3`\t|  integer \t|\n","|  5 \t|  `Product_Info_4`|\treal   \t|\n","|  6 \t|  `Product_Info_5`|\tinteger 2, 3\t|\n","|  7 \t|   `Product_Info_6`|\tinteger 1, 3   \t|\n","|  8 \t|   `Product_Info_7`|\tinteger 1, 2, 3   \t|\n","|  9 \t|   `Ins_Age`|\treal   \t|\n","|  10 \t|   `Ht`|\treal   \t|\n","|  11 \t|   `Wt`|\treal    |\n","|  12 \t|   `BMI`|\treal\t|\n","|  13 \t|   `Medical_Keyword_`|\tint 1/0   \t|\n","|  14 \t|   `Response`|\tinteger 1 -> 8   \t|\n","\n","### Description of the data\n","* `id`: A unique identifier associated with an application.\n","* `Product_Info_1_to_7`: A set of normalized variables relating to the product applied for\n","* `Ins_Age`: Normalized age of applicant\n","* `Ht`: Normalized height of applicant\n","* `Wt`: Normalized weight of applicant\n","* `BMI`: Normalized BMI of applicant. Body mass index (BMI) is a measure of body fat based on height and weight.\n","\n","* `Employment_Info_1-6`: A set of normalized variables relating to the employment history of the applicant.\n","\n","* `InsuredInfo_1-6`: A set of normalized variables providing information about the applicant.\n","* `Insurance_History_1-9`: A set of normalized variables relating to the insurance history of the applicant.\n","\n","* `Family_Hist_1-5`: A set of normalized variables relating to the family history of the applicant.\n","\n","* `Medical_Keyword_`: A set of dummy variables relating to the presence of/absence of a medical keyword being associated with the application.\n","* `Response`: This is the target variable, an ordinal variable relating to the final decision associated with an application "]},{"cell_type":"code","metadata":{"id":"fYycWj_VzwKS","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","%matplotlib inline\n","# Importations \n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","from pandas_profiling import ProfileReport\n","\n","sns.set()\n","\n","#pd.set_option('display.max_columns', None)  # or 1000\n","#pd.set_option('display.max_rows', None)  # or 1000\n","#pd.set_option('display.max_colwidth', -1)  # or 199"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chJdzbNvzwKX","colab_type":"text"},"source":["## 1 Acquire the data\n"]},{"cell_type":"markdown","metadata":{"id":"ZB3xZzvUzwKY","colab_type":"text"},"source":["### 1.1 Read  CSV\n","\n","**Q:** Load the [data] in the current path and display the head: the name of the file is trainCourse.csv."]},{"cell_type":"code","metadata":{"id":"3SA3oD4PzwKY","colab_type":"code","colab":{}},"source":["df = pd.read_csv('trainCourse.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IO1M66sDzwKc","colab_type":"text"},"source":["**Q:** How many samples and features that we have in the dataset"]},{"cell_type":"code","metadata":{"id":"F4QeNLdVzwKd","colab_type":"code","colab":{}},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-j3Dgv8IzwKh","colab_type":"text"},"source":["### 1.2 Describing data\n","\n","\n","**Q:** Which features are available in the dataset?\n"]},{"cell_type":"code","metadata":{"id":"GeBkfoG-zwKh","colab_type":"code","colab":{}},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J08lZ5jzzwKl","colab_type":"text"},"source":["**Q:** Print the global information about the data and the different feature types "]},{"cell_type":"code","metadata":{"id":"tBH5X8okzwKm","colab_type":"code","colab":{}},"source":["df.info(verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gONQNlOzwKp","colab_type":"text"},"source":["**Q:** Which features are categorical?"]},{"cell_type":"code","metadata":{"id":"qOOI2lzezwKq","colab_type":"code","colab":{}},"source":["df.describe(include=['O'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vl4T0dlFzwKu","colab_type":"code","colab":{}},"source":["# Select columns of type 'O'\n","df.select_dtypes(include='O')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eiCgdtCWzwKy","colab_type":"text"},"source":["**Q:** Which features are numerical?"]},{"cell_type":"code","metadata":{"id":"YJSTslm9zwKy","colab_type":"code","colab":{}},"source":["df.describe(exclude='O')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SgYm17b4zwK7","colab_type":"code","colab":{}},"source":["df.select_dtypes(exclude='O')\n","\n","#df.describe(include=[np.number])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TxEVlL5AzwK-","colab_type":"text"},"source":["**Q:** The Id Variable is it a variable that you can use in machine learning? Print the number of unique value of Id"]},{"cell_type":"code","metadata":{"id":"NNoVwI1czwK_","colab_type":"code","colab":{}},"source":["df['Id'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIfFd6bvzwLD","colab_type":"code","colab":{}},"source":["len(df.Id.unique())\n","df['Id'].nunique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J7csqCwbzwLG","colab_type":"text"},"source":["5938 instances and all of them are unique so we can drop this variable and for that we will use the **drop** function from Pandas"]},{"cell_type":"code","metadata":{"id":"PeDwUN2fzwLH","colab_type":"code","colab":{}},"source":["df = df.drop('Id',axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUFClmKpzwLK","colab_type":"code","colab":{}},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"bYJHbTkszwLN","colab_type":"text"},"source":["### 1.3 Exploration"]},{"cell_type":"markdown","metadata":{"id":"oYlw5zxHzwLO","colab_type":"text"},"source":["**Q:** **What is the distribution of numerical feature values across the samples?"]},{"cell_type":"code","metadata":{"id":"o8hVYoKFzwLO","colab_type":"code","colab":{}},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZmPNru_zwLR","colab_type":"text"},"source":["**Q:** are there missing values in the data? How many? give the ratio for each feature"]},{"cell_type":"code","metadata":{"id":"ZiuAiZhvzwLS","colab_type":"code","colab":{}},"source":["# Way 1 : no nan data\n","df.notnull().sum()[df.notnull().sum() < df.shape[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwuKxYbfzwLW","colab_type":"code","colab":{}},"source":["#Way 2 : only nan data\n","df.isnull().sum()[df.isnull().sum() >0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aI6K4QMBzwLZ","colab_type":"code","colab":{}},"source":["# Ratio\n","len(df.isnull().sum()[df.isnull().sum() >0]/df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MmYJnjakzwLb","colab_type":"text"},"source":["**Q:** Now let's look at the variable that we are interested in predicting ** the Y variable ** which is **Response**. Using the count function, display the class imbalance"]},{"cell_type":"code","metadata":{"id":"TD9D-MjfzwLc","colab_type":"code","colab":{}},"source":["df.Response.value_counts()/df.shape[0]\n","df['Response'].value_counts(normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"848y45NhzwLf","colab_type":"text"},"source":["##  2 Exploratory Data Analysis\n","\n","We arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n","\n","* **Correlating.**: We want to know how well does each feature correlate with Survival. \n","* **Completing.** : We want to complete missing values\n","* **Correcting.**: We want de drop some features\n","* **Creating.**: We want to creat new features\n"]},{"cell_type":"markdown","metadata":{"id":"YCKXJvN_zwLf","colab_type":"text"},"source":["**Q:** Draw a big picture of the data using ProfileReport"]},{"cell_type":"code","metadata":{"id":"RDWspEmozwLg","colab_type":"code","colab":{}},"source":["profile = ProfileReport(df, title='Life Insurance', html={'style':{'full_width':True}})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXYIwLOWzwLj","colab_type":"code","colab":{}},"source":["profile.to_widgets()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UGSXrD-xzwLl","colab_type":"text"},"source":["It can be very interesting to look at the *distribution* of our variables, comparing them to the *output variable*.\n","\n","Here we plot it using the seaborn library."]},{"cell_type":"code","metadata":{"id":"YKlaEhHQzwLm","colab_type":"code","colab":{}},"source":["features = df.columns\n","\n","def PlotDistributiontest(X_train,Y_train,nb_of_features=20):\n","    plt.figure(figsize=(12,nb_of_features*4))\n","    gs = gridspec.GridSpec(nb_of_features, 2) #Customizing Figure Layouts Using GridSpec \n","    for i in range(nb_of_features):\n","        \n","        ax = plt.subplot(gs[i,0])\n","        try :\n","            sns.distplot(X_train[Y_train == 0][X_train.columns[i]], bins=50,color='red') \n","            ax.set_xlabel('')\n","            ax.set_title('histogram of feature of risk 0: ' + str(X_train.columns[i]))\n","        except :\n","            print ('erreur')\n","\n","        try :\n","            ax = plt.subplot(gs[i,1])\n","            sns.distplot(X_train[Y_train == 1][X_train.columns[i]], bins=50,color='green')\n","            ax.set_xlabel('')\n","            ax.set_title('histogram of feature of risk 1: ' + str(X_train.columns[i]))\n","        except: \n","            print('erreur')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsXIxSCUzwLp","colab_type":"code","colab":{}},"source":["PlotDistributiontest(df.drop('Product_Info_2', axis=1),df['Response'],nb_of_features=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCbAWfK4zwLs","colab_type":"code","colab":{}},"source":["def PlotDistribution(X_train,Y_train,nb_of_features=20):\n","    plt.figure(figsize=(12,nb_of_features*4))\n","    gs = gridspec.GridSpec(nb_of_features, 1) #Customizing Figure Layouts Using GridSpec \n","    for i in range(nb_of_features):\n","        \n","        try :\n","            ax = plt.subplot(gs[i])\n","            sns.distplot(X_train[Y_train == 0][X_train.columns[i]], bins=50,color='red') \n","            sns.distplot(X_train[Y_train == 1][X_train.columns[i]], bins=50,color='green')\n","            ax.set_xlabel('')\n","            ax.set_title('histogram of feature of risk: ' + str(X_train.columns[i]))\n","        except :\n","            print ('erreur')\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLwcyPNYzwLv","colab_type":"code","colab":{}},"source":["PlotDistribution(df.drop('Product_Info_2', axis=1),df['Response'],nb_of_features=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-tqD2qFzwLy","colab_type":"text"},"source":["## 3 Preparing the data"]},{"cell_type":"markdown","metadata":{"id":"OBgmic-XzwLy","colab_type":"text"},"source":["### 3.1 Splitting the Dataset into Training set and Test Set"]},{"cell_type":"markdown","metadata":{"id":"dQiblJS1zwLz","colab_type":"text"},"source":["**Q:** Splitting the dataset between X and y"]},{"cell_type":"code","metadata":{"id":"Qo7Alm2SzwLz","colab_type":"code","colab":{}},"source":["y = df[\"Response\"]\n","X = df.drop([\"Response\"],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DW34xzCHzwL2","colab_type":"text"},"source":["**Q:** Split the data between train 80%  and test 20% in stratified manner (X_train, X_test, y_train, y_test)"]},{"cell_type":"code","metadata":{"id":"LInvGuuxzwL2","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l-qJsMU_zwL6","colab_type":"text"},"source":["### 3.2  Sampling and co-variance shift"]},{"cell_type":"markdown","metadata":{"id":"O3IJgYR_zwL6","colab_type":"text"},"source":["We have now a training set of instances, and a test set and we have Labels and the name of the **Y** variable let's taka a closer look. \n","\n","**Q:** Look at the class imbalance difference between the test and train set ."]},{"cell_type":"code","metadata":{"id":"KVLOXnhxzwL7","colab_type":"code","colab":{}},"source":["y_train.value_counts()/X_train.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"89bA_BkozwL-","colab_type":"code","colab":{}},"source":["y_test.value_counts()/X_test.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOFe_7JZzwMA","colab_type":"text"},"source":["**Q:** Check that there isn't a co-variance shift between the train and the test."]},{"cell_type":"code","metadata":{"id":"MI1F9lmnzwMB","colab_type":"code","colab":{}},"source":["sns.distplot(X_train['Product_Info_4'],bins=50)\n","sns.distplot(X_test['Product_Info_4'],bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULq50-uJzwMD","colab_type":"code","colab":{}},"source":["# Step 1 :  remove nan column\n","\n","train_sample = X_train.copy().dropna(axis=1) # seulement les features sans nan\n","train_sample['Target'] = 1\n","test_sample = X_test.copy().dropna(axis=1)\n","test_sample['Target'] = 0\n","\n","cov_shift = pd.concat([train_sample,test_sample], axis=0,ignore_index=True)\n","y_shift = cov_shift['Target']\n","cov_shift = cov_shift.drop('Target',axis=1)\n","cov_shift = cov_shift.drop('Product_Info_2',axis=1) # categorical\n","\n","\n","train_sample.shape,test_sample.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88EE_fe9zwMG","colab_type":"code","colab":{}},"source":["# Step 2 : Run a model to check if we can make a difference between the train and test\n","from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n","from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n","from sklearn.ensemble import RandomForestClassifier\n","\n","features = cov_shift.columns\n","\n","clf =  RandomForestClassifier(n_estimators=150, max_depth=2)\n","\n","predictions = np.zeros(y_shift.shape)\n","\n","for feature in features : \n","    cv = StratifiedKFold(n_splits=20,shuffle=True)\n","\n","    for fold, (train_idx, test_idx) in enumerate(cv.split(cov_shift,y_shift)):\n","        X_train_s, X_test_s = cov_shift.loc[train_idx], cov_shift.loc[test_idx]\n","        y_train_s, y_test_s = y_shift[train_idx], y_shift[test_idx]\n","\n","        clf.fit(X_train_s[[feature]], y_train_s)\n","        probs = clf.predict_proba(X_test_s[[feature]])[:, 1]\n","        predictions[test_idx] = probs\n","    \n","    print ('Feature {}: ROC-AUC {}'.format(feature, roc_auc_score(y_shift, predictions)))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBmDHuLizwMJ","colab_type":"text"},"source":["### 3.3 Data encoding"]},{"cell_type":"markdown","metadata":{"id":"bJHg3BREzwMJ","colab_type":"text"},"source":["We have seen one hot encoding (or creation of dummy variables) let 's use a sickit learn package to do the trick. Remember our categorical variable Product_Info_2:"]},{"cell_type":"markdown","metadata":{"id":"7G7vvy04zwMK","colab_type":"text"},"source":["**Q:** Print the different modality of this variable"]},{"cell_type":"code","metadata":{"id":"yfkLghTwzwMK","colab_type":"code","colab":{}},"source":["X_train['Product_Info_2'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wEZVLjTJzwMN","colab_type":"text"},"source":["We will now create 19 dummy variables oner per modality of the Product_Info_2 variable and populate those with 0 and 1 accoridng to the instances. There are several way for doing this.  \n","\n","* we can use OneHotEncoder or MultiLabelBinarizer or LabelBinarizer or get_dummies\n","* https://chrisalbon.com/machine_learning/preprocessing_structured_data/one-hot_encode_features_with_multiple_labels/\n","* https://stackoverflow.com/questions/50473381/scikit-learns-labelbinarizer-vs-onehotencoder\n","\n","**Q:** Use any method for encoding the labels of Product_Info_2 feature: . Place those variables in a separate dataframe X_train_dum, and X_test_dum. Also print the head the obtained result"]},{"cell_type":"code","metadata":{"id":"xbOQrBbizwMN","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelBinarizer\n","\n","encoder = LabelBinarizer()\n","encoder.fit(X_train['Product_Info_2'])\n","X_train_dum = encoder.transform(X_train['Product_Info_2'])\n","X_test_dum = encoder.transform(X_test['Product_Info_2'])\n","\n","X_train_dum = pd.DataFrame(data=X_train_dum)\n","X_test_dum = pd.DataFrame(data=X_test_dum)\n","\n","\n","X_train_dum.head()\n","\n","encoder.classes_\n","#TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDDNm2Z_zwMQ","colab_type":"text"},"source":["We now have 19 columns labeled 0 through 18 which we are going to quickly rename using a for loop function. \n","\n","**Q:** Use a list to rename the columns of X_train_dum, and X_test_dum as 'Product_Info_2_' + 'modality'"]},{"cell_type":"code","metadata":{"id":"-V2jatDGzwMQ","colab_type":"code","colab":{}},"source":["X_train_dum.columns = ['Product_Info_2_'+mode for mode in encoder.classes_]\n","X_test_dum.columns = ['Product_Info_2_'+mode for mode in encoder.classes_]\n","X_train_dum.index = X_train.index\n","X_test_dum.index = X_test.index\n","\n","X_train_dum.head()\n","X_train = X_train.join(X_train_dum)\n","X_train = X_train.drop('Product_Info_2',axis=1)\n","X_train.sample()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I8pHg2yhzwMT","colab_type":"text"},"source":["**Q:** Add the X_train_dum, and X_test_dum to the orginal data and remove the categorical variable using pandas join function"]},{"cell_type":"code","metadata":{"id":"KHdkEIE-zwMU","colab_type":"code","colab":{}},"source":["X_train = X_train.join(X_train_dum)\n","X_train = X_train.drop('Product_Info_2',axis=1)\n","X_train.sample()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKk_TgIMzwMY","colab_type":"text"},"source":["### 3.4 Data imputaion"]},{"cell_type":"markdown","metadata":{"id":"3ZwH4aRWzwMY","colab_type":"text"},"source":["Fast and classic Imputation transformer for completing missing values.\n","* https://scikit-learn.org/stable/modules/impute.html\n","\n","**Q:** use impute to fill missing values (before check of missing values)"]},{"cell_type":"code","metadata":{"id":"mDxfkGtbzwMZ","colab_type":"code","colab":{}},"source":["## Check if there are any missing values\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQOwBVqJzwMd","colab_type":"code","colab":{}},"source":["## Way 1 : Impute your values using pandas (fillna)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAtgKqVYzwMh","colab_type":"code","colab":{}},"source":["## Way 2 : Impute using SimplteImputer\n","\n","from sklearn.impute import SimpleImputer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"388DFnLLzwMk","colab_type":"text"},"source":["### 3.5 Scaling the data "]},{"cell_type":"markdown","metadata":{"id":"VdRXoPcSzwMl","colab_type":"text"},"source":["**Q:** Normalizing by the range of the data Min: MinMaxScaler"]},{"cell_type":"code","metadata":{"id":"UkFJzzk8zwMl","colab_type":"code","colab":{}},"source":["# X = (X - min(X)) / (max(X) - min(X))    range between 0 and 1\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","\n","X_train_minmax = \n","X_test_minmax = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A-ii-Xl5zwMo","colab_type":"text"},"source":["#### b) StandardScaler : Standardize features by removing the mean and scaling to unit variance"]},{"cell_type":"code","metadata":{"id":"GaM7LDkXzwMp","colab_type":"code","colab":{}},"source":["# X = (X - mean(X)) / std(X)   range between 0 and 1\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","\n","X_train_std = scaler.transform(X_train)\n","X_test_std = scaler.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQxJp1iJzwMr","colab_type":"text"},"source":["### 4. Model Baseline"]},{"cell_type":"code","metadata":{"id":"UX5DEIW6zwMs","colab_type":"code","colab":{}},"source":["X_train.shape,y_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l37HQvywzwMu","colab_type":"code","colab":{}},"source":["#Evaluate metric(s) by cross-validation and also record fit/score times. Use Decision tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import cross_validate\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DINEvYc9zwMx","colab_type":"code","colab":{}},"source":["cv_dt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTTRC185zwM0","colab_type":"code","colab":{}},"source":["print(\"Mean score: \", cv_dt['test_score'].mean(), \"Mean std: \", cv_dt['test_score'].std())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EIZXCWNzwM2","colab_type":"text"},"source":["### 5. Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"oEduBR3rzwM2","colab_type":"text"},"source":["#### 5.1 Pearson Correlation\n","\n","Pearson's correlation coefficient is the covariance of the two variables divided by the product of their standard deviations.\n","\n","$$p_{X,Y} = \\frac{cov(X,Y)}{sigma_{X}sigma_{Y}}$$\n","\n","$$ \\text{ where cov is the covariance,   }  sigma_{X} \\text{  is the standard deviation of X and }  sigma_{Y} \\text{  is the standard deviation of Y} $$"]},{"cell_type":"code","metadata":{"id":"qx0T1ysIzwM3","colab_type":"code","colab":{}},"source":["# Pearson Correlation\n","from scipy.stats import pearsonr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvkLIT2zzwM6","colab_type":"text"},"source":["** NB **\n","The function below computes the pearson correlation using pearsonr"]},{"cell_type":"code","metadata":{"id":"8tpdhsntzwM6","colab_type":"code","colab":{}},"source":["def TabPearsonr(X_train,y_train):\n","    \"\"\"\n","    Computes the pearson correlation between each column of X_train and the column Y_train.\n","    \n","    Arguments:\n","    X_train -- learning set\n","    Y_train -- learning output\n","    Labels -- names of the features\n","    \n","    Returns:\n","    TabResultsPearson -- the table of the normalized mutual informations such as\n","    TabResultsPearson[j] = PearsonCorr(X_train[:,i],Y_train)\n","    \"\"\"\n","    ListPearsonCorr = []\n","    for feature in features:\n","        ListPearsonCorr.append(pearsonr(X_train[feature],y_train))\n","    \n","    TabResultsPearson = pd.DataFrame(ListPearsonCorr,columns=['Pearson Correlation','P-Value'],index=features)\n","    #TabResultsPearson = TabResultsPearson.transpose()\n","    return TabResultsPearson\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsB192iVzwM_","colab_type":"code","colab":{}},"source":["TabResultsPearson = TabPearsonr(X_train,y_train)\n","TabResultsPearson"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ueYcSLjzwNB","colab_type":"code","colab":{}},"source":["# https://dataschool.com/fundamentals-of-analysis/correlation-and-p-value/\n","def SelectBestFeatures(TabResultsPearson,thresPVal=0.05):\n","    \"\"\"\n","    Extracts the features that are correlated the most to Y_train and that the pValue is under thresPVal.\n","    \n","    Arguments:\n","    TabResultsPearson -- table of the correlations\n","    thresPVal -- pValue threshold\n","    \n","    Returns:\n","    ListSelectedFeatures -- List of selected features\n","    \"\"\"\n","    \n","    potentiel = TabResultsPearson[TabResultsPearson['P-Value']< thresPVal]\n","    MAD = (potentiel.abs() - potentiel.abs().median()).abs().median()\n","    print (potentiel)\n","    #IdxCols = (TabResultsPearson.loc['P-Value',:] < thresPVal).nonzero()[0]\n","    #TabResultsPearsonLoc = TabResultsPearson.iloc['Pearson Correlation',IdxCols]\n","    \n","    #MAD = (TabResultsPearsonLoc.abs() \\    - TabResultsPearsonLoc.abs().median()).abs().median()\n","    #ListSelectedFeatures = TabResultsPearsonLoc.loc[TabResultsPearsonLoc.abs() > TabResultsPearsonLoc.abs().median() + MAD]\n","    #ListSelectedFeatures = ListSelectedFeatures.index\n","    #ListSelectedFeatures = ListSelectedFeatures.values\n","    #return ListSelectedFeatures"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7UcjIeJzwNG","colab_type":"code","colab":{}},"source":["ListSelectedFeatures = SelectBestFeatures(TabResultsPearson,thresPVal=0.05)\n","\n","print(\"Selected Features : \")\n","print(ListSelectedFeatures)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6RI_qDzzwNI","colab_type":"code","colab":{}},"source":["X_train2 = pd.DataFrame(X_train2,columns=Cols)\n","X_test2 = pd.DataFrame(X_test2,columns=Cols)\n","\n","newX_train = X_train2[ListSelectedFeatures].copy()\n","newX_test = X_test2[ListSelectedFeatures].copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkOpzCKOzwNK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yPhcxCvQzwNM","colab_type":"text"},"source":["#### 2.2 Wrapped method\n"]},{"cell_type":"markdown","metadata":{"id":"vQHiwwXjzwNN","colab_type":"text"},"source":["#### Recursive Feature Elimination"]},{"cell_type":"markdown","metadata":{"id":"AcEmLFjmzwNN","colab_type":"text"},"source":["The RFE procedure removes one by one the less significative variables using the chosen learning model; in this case, we use the random forests."]},{"cell_type":"code","metadata":{"id":"dv89ajZmzwNN","colab_type":"code","colab":{}},"source":["from sklearn.feature_selection import RFE\n","from sklearn.ensemble import RandomForestClassifier\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdFUBK5ZzwNP","colab_type":"code","colab":{}},"source":["sortedIdSelected = np.argsort(tf.ranking_)\n","sortedCols = newX_train.columns[sortedIdSelected]\n","newX_train[sortedCols].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1j9-iA5HzwNS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t_XqJW2rzwNU","colab_type":"code","colab":{}},"source":["from sklearn.metrics import mean_squared_error, r2_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RtdS60QBzwNW","colab_type":"code","colab":{}},"source":["def modelfit(X_train,Y_train,X_test,Y_test,ListSelectFeatures,Labels):\n","    \"\"\"\n","    Computes the 200-trees RandomForest fitted model on Y_train using the selected features.\n","    Then predicts on X_test.\n","    \n","    Arguments:\n","    X_train -- learning set\n","    Y_train -- learning output\n","    X_test -- test set\n","    Y_test -- test output\n","    ListSelectFeatures -- list of selected features\n","    Labels -- names of the features\n","    \n","    Returns:\n","    pred -- prediction vector\n","    MSE -- mean squared error\n","    R2 -- R2 score\n","    \n","    NB :\n","    model = RFModel.fit(X_train,Y_train)\n","    pred = model.predict(X_test)\n","    \"\"\"\n","    \n","    print(\"MSE=\",mean_squared_error(pred,Y_test))\n","    print(\"R2=\",r2_score(Y_test,pred))\n","    return pred,mean_squared_error(pred,Y_test), r2_score(Y_test,pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66yKVpFxzwNZ","colab_type":"text"},"source":["Dummy example :"]},{"cell_type":"code","metadata":{"id":"3t8DpJekzwNa","colab_type":"code","colab":{}},"source":["## Example\n","\n","LabelsRand = range(50)\n","matRand_Train = np.array([[1.0] * 30 + [0.0] * 20] * 50 + [[1.0] * 30 + [0.0] * 20] * 50)\n","yRand_Train = np.array([1] * 50 + [0] * 50)\n","matRand_Test = np.array([[0.0] * 25 + [1.0] * 25] * 40)\n","yRand_Test = np.array([0] * 20 + [0] * 20)\n","print(matRand_Train.shape)\n","ListSelectFeatures = range(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4M9HmlkzwNc","colab_type":"code","colab":{}},"source":["modelfit(matRand_Train,yRand_Train,matRand_Test,yRand_Test,range(15),LabelsRand);\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n64gWFoFzwNd","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}